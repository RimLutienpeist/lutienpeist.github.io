> Not what youâ€™ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection

## ABSTRACT

The functionalities of recent LLMs can be flexibly modulated via natural language prompts. This renders them susceptible to targeted adversarial prompting, e.g., Prompt Injection (PI) attacks enable attackers to override original instructions and employed controls. 

We derive a comprehensive taxonomy from a computer security perspective to systematically investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other novel security risks.

## KEYWORDS